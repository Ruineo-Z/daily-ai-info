"""è¶…è½»é‡çº§é™æ€ç½‘ç«™ç”Ÿæˆå™¨ - ä¸“ä¸º2æ ¸2GæœåŠ¡å™¨ä¼˜åŒ–"""

import os
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
from jinja2 import Environment, FileSystemLoader
from loguru import logger
from .config import PROJECT_ROOT


class StaticSiteGenerator:
    """é™æ€ç½‘ç«™ç”Ÿæˆå™¨ï¼Œç”Ÿæˆè½»é‡çº§HTMLç½‘ç«™"""

    def __init__(self, output_dir: str = "dist"):
        self.project_root = Path(PROJECT_ROOT)
        self.templates_dir = self.project_root / "app" / "templates"
        self.output_dir = Path(output_dir)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–Jinja2æ¨¡æ¿å¼•æ“
        self.env = Environment(
            loader=FileSystemLoader(str(self.templates_dir)),
            trim_blocks=True,
            lstrip_blocks=True
        )

        logger.info(f"é™æ€ç½‘ç«™ç”Ÿæˆå™¨åˆå§‹åŒ–: è¾“å‡ºç›®å½• {self.output_dir}")

    def generate_site(self, reports_data: List[Dict[str, Any]],
                     latest_summary: Dict[str, Any]) -> bool:
        """ç”Ÿæˆå®Œæ•´çš„é™æ€ç½‘ç«™"""
        try:
            logger.info("å¼€å§‹ç”Ÿæˆé™æ€ç½‘ç«™")

            # 1. æ¸…ç†æ—§æ–‡ä»¶ä½†ä¿ç•™ç»“æ„
            self._clean_output_dir()

            # 2. å¤„ç†æ•°æ®
            processed_data = self._process_reports_data(reports_data)

            # 3. ç”Ÿæˆé¦–é¡µ
            self._generate_index_page(processed_data, latest_summary)

            # 4. ç”Ÿæˆå†å²å½’æ¡£é¡µ
            self._generate_archive_page(processed_data)

            # 5. ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šé¡µé¢
            self._generate_report_pages(reports_data)

            # 6. ç”Ÿæˆå…³äºé¡µé¢
            self._generate_about_page()

            # 7. ç”ŸæˆAPIæ•°æ®æ–‡ä»¶
            self._generate_api_data(processed_data)

            # 8. åˆ›å»ºrobots.txtå’Œsitemap
            self._generate_seo_files(processed_data)

            logger.info("âœ… é™æ€ç½‘ç«™ç”Ÿæˆå®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"é™æ€ç½‘ç«™ç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"é™æ€ç½‘ç«™ç”Ÿæˆå¤±è´¥: {e}") from e

    def _clean_output_dir(self):
        """æ¸…ç†è¾“å‡ºç›®å½•ï¼Œä½†ä¿ç•™å¿…è¦çš„ç»“æ„"""
        if self.output_dir.exists():
            # åªåˆ é™¤HTMLæ–‡ä»¶å’ŒAPIæ–‡ä»¶ï¼Œä¿ç•™å¯èƒ½çš„é™æ€èµ„æº
            for item in self.output_dir.iterdir():
                if item.is_file() and item.suffix in ['.html', '.json', '.xml', '.txt']:
                    item.unlink()
                elif item.is_dir() and item.name in ['reports', 'api']:
                    shutil.rmtree(item)

    def _process_reports_data(self, reports_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¤„ç†æŠ¥å‘Šæ•°æ®ï¼Œç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not reports_data:
            return {
                "total_reports": 0,
                "total_projects": 0,
                "total_trends": 0,
                "active_days": 0,
                "years": [],
                "reports_by_year": {},
                "recent_reports": [],
                "language_stats": {},
                "total_stars_today": 0
            }

        # æŒ‰æ—¶é—´æ’åº
        sorted_reports = sorted(reports_data, key=lambda x: x.get('date', ''), reverse=True)
        recent_reports = sorted_reports[:7]  # æœ€è¿‘7å¤©

        # ç»Ÿè®¡ä¿¡æ¯
        total_projects = sum(len(report.get('projects', [])) for report in reports_data)
        total_trends = sum(len(report.get('trends', [])) for report in reports_data)

        # æŒ‰å¹´æœˆåˆ†ç»„
        reports_by_year = {}
        years = set()

        for report in sorted_reports:
            date_str = report.get('date', '')
            if not date_str:
                continue

            try:
                date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                year = str(date_obj.year)
                month = f"{date_obj.month:02d}"

                years.add(year)

                if year not in reports_by_year:
                    reports_by_year[year] = {}
                if month not in reports_by_year[year]:
                    reports_by_year[year][month] = []

                # æ·»åŠ URLå’Œå…¶ä»–å…ƒæ•°æ®
                report_with_meta = report.copy()
                report_with_meta['url'] = f"/reports/{date_str}.html"
                report_with_meta['projects_count'] = len(report.get('projects', []))
                report_with_meta['trends_count'] = len(report.get('trends', []))

                reports_by_year[year][month].append(report_with_meta)

            except ValueError:
                logger.warning(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {date_str}")
                continue

        # è¯­è¨€ç»Ÿè®¡
        language_stats = {}
        total_stars_today = 0

        for report in reports_data:
            for project in report.get('projects', []):
                lang = project.get('language', 'Unknown')
                language_stats[lang] = language_stats.get(lang, 0) + 1

                # ç»Ÿè®¡ä»Šæ—¥æ–°å¢Stars
                stars_today_str = str(project.get('stars_today', '')).replace(',', '')
                try:
                    if stars_today_str.isdigit():
                        total_stars_today += int(stars_today_str)
                except (ValueError, TypeError):
                    pass

        # æ’åºè¯­è¨€ç»Ÿè®¡
        sorted_language_stats = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            "total_reports": len(reports_data),
            "total_projects": total_projects,
            "total_trends": total_trends,
            "active_days": len(reports_data),
            "years": sorted(years, reverse=True),
            "reports_by_year": reports_by_year,
            "recent_reports": [
                {
                    "date": r.get('date', ''),
                    "url": f"/reports/{r.get('date', '')}.html",
                    "projects_count": len(r.get('projects', []))
                } for r in recent_reports
            ],
            "language_stats": sorted_language_stats,
            "total_stars_today": total_stars_today
        }

    def _generate_index_page(self, processed_data: Dict[str, Any],
                           latest_summary: Dict[str, Any]):
        """ç”Ÿæˆé¦–é¡µ"""
        template = self.env.get_template('index.html')

        # å‡†å¤‡æœ€æ–°çš„é¡¹ç›®æ•°æ®
        latest_projects = []
        if processed_data['recent_reports']:
            # ä»æœ€æ–°æŠ¥å‘Šä¸­è·å–é¡¹ç›®
            latest_date = processed_data['recent_reports'][0]['date']
            for report in self._load_reports_data():
                if report.get('date') == latest_date:
                    latest_projects = report.get('projects', [])[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
                    break

        # æ·»åŠ AIæ‘˜è¦åˆ°é¡¹ç›®ä¸­
        for i, project in enumerate(latest_projects):
            if 'project_summaries' in latest_summary and i < len(latest_summary['project_summaries']):
                project['ai_summary'] = latest_summary['project_summaries'][i]

        content = template.render(
            current_page='index',
            last_update=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            summary=latest_summary.get('summary', ''),
            trends=latest_summary.get('trends', []),
            projects=latest_projects,
            recent_reports=processed_data['recent_reports'],
            language_stats=processed_data['language_stats'],
            total_stars_today=processed_data['total_stars_today']
        )

        output_file = self.output_dir / "index.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"âœ… é¦–é¡µç”Ÿæˆå®Œæˆ: {output_file}")

    def _generate_archive_page(self, processed_data: Dict[str, Any]):
        """ç”Ÿæˆå†å²å½’æ¡£é¡µé¢"""
        template = self.env.get_template('archive.html')

        content = template.render(
            current_page='archive',
            **processed_data
        )

        output_file = self.output_dir / "archive.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"âœ… å½’æ¡£é¡µé¢ç”Ÿæˆå®Œæˆ: {output_file}")

    def _generate_report_pages(self, reports_data: List[Dict[str, Any]]):
        """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šé¡µé¢"""
        template = self.env.get_template('report.html')
        reports_dir = self.output_dir / "reports"
        reports_dir.mkdir(exist_ok=True)

        for report in reports_data:
            date_str = report.get('date', '')
            if not date_str:
                continue

            # å¤„ç†é¡¹ç›®æ•°æ®ï¼Œæ·»åŠ AIæ‘˜è¦
            projects = report.get('projects', [])
            project_summaries = report.get('project_summaries', [])

            for i, project in enumerate(projects):
                if i < len(project_summaries):
                    project['ai_summary'] = project_summaries[i]

            # è®¡ç®—è¯­è¨€ç»Ÿè®¡
            language_stats = {}
            total_stars_today = 0

            for project in projects:
                lang = project.get('language', 'Unknown')
                language_stats[lang] = language_stats.get(lang, 0) + 1

                stars_today_str = str(project.get('stars_today', '')).replace(',', '')
                try:
                    if stars_today_str.isdigit():
                        total_stars_today += int(stars_today_str)
                except (ValueError, TypeError):
                    pass

            sorted_language_stats = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)

            content = template.render(
                current_page='report',
                report_title=f"GitHub Trending AIæŠ€æœ¯åˆ†ææŠ¥å‘Š - {date_str}",
                generation_time=report.get('generation_time', date_str),
                summary=report.get('summary', ''),
                trends=report.get('trends', []),
                projects=projects,
                language_stats=sorted_language_stats,
                total_stars_today=total_stars_today
            )

            output_file = reports_dir / f"{date_str}.html"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)

        logger.info(f"âœ… {len(reports_data)} ä¸ªæŠ¥å‘Šé¡µé¢ç”Ÿæˆå®Œæˆ")

    def _generate_about_page(self):
        """ç”Ÿæˆå…³äºé¡µé¢"""
        about_content = """
        <div class="card">
            <div class="card-header">
                <h1 class="card-title">å…³äºæœ¬é¡¹ç›®</h1>
            </div>
            <div class="card-body">
                <h2>é¡¹ç›®ç®€ä»‹</h2>
                <p>æ¯æ—¥AIæŠ€æœ¯è¶‹åŠ¿åˆ†ææ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„AIæŠ€æœ¯èµ„è®¯åˆ†æç³»ç»Ÿï¼Œé€šè¿‡çˆ¬å–GitHub Trendingæ•°æ®ï¼Œç»“åˆGoogle Gemini AIè¿›è¡Œæ™ºèƒ½åˆ†æï¼Œä¸ºå¼€å‘è€…æä¾›æœ€æ–°çš„AIæŠ€æœ¯åŠ¨æ€å’Œè¶‹åŠ¿æ´å¯Ÿã€‚</p>

                <h2>æŠ€æœ¯æ¶æ„</h2>
                <ul>
                    <li><strong>æ•°æ®æ¥æº</strong>: GitHub Trending API</li>
                    <li><strong>AIåˆ†æ</strong>: Google Gemini 2.5 Pro</li>
                    <li><strong>è°ƒåº¦ç³»ç»Ÿ</strong>: APScheduler (æ¯æ—¥8:30è‡ªåŠ¨æ‰§è¡Œ)</li>
                    <li><strong>ç½‘ç«™ç”Ÿæˆ</strong>: é™æ€HTMLé¡µé¢ï¼Œä¼˜åŒ–åŠ è½½é€Ÿåº¦</li>
                </ul>

                <h2>åŠŸèƒ½ç‰¹ç‚¹</h2>
                <ul>
                    <li>ğŸ” è‡ªåŠ¨çˆ¬å–GitHubçƒ­é—¨AIé¡¹ç›®</li>
                    <li>ğŸ¤– AIæ™ºèƒ½å»é‡å’Œå†…å®¹åˆ†æ</li>
                    <li>ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿è¯†åˆ«å’Œæ€»ç»“</li>
                    <li>ğŸ“± å“åº”å¼è®¾è®¡ï¼Œæ”¯æŒç§»åŠ¨è®¾å¤‡</li>
                    <li>ğŸš€ æé€ŸåŠ è½½ï¼Œä¼˜åŒ–ç”¨æˆ·ä½“éªŒ</li>
                </ul>

                <h2>è”ç³»æ–¹å¼</h2>
                <p>å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬ã€‚</p>
            </div>
        </div>
        """

        template = self.env.get_template('base.html')
        content = template.render(
            current_page='about',
            page_title='å…³äºæœ¬é¡¹ç›®'
        ).replace('{% block content %}{% endblock %}', about_content)

        output_file = self.output_dir / "about.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"âœ… å…³äºé¡µé¢ç”Ÿæˆå®Œæˆ: {output_file}")

    def _generate_api_data(self, processed_data: Dict[str, Any]):
        """ç”ŸæˆAPIæ•°æ®æ–‡ä»¶"""
        api_dir = self.output_dir / "api"
        api_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆæ±‡æ€»API
        api_data = {
            "stats": {
                "total_reports": processed_data["total_reports"],
                "total_projects": processed_data["total_projects"],
                "total_trends": processed_data["total_trends"],
                "active_days": processed_data["active_days"]
            },
            "recent_reports": processed_data["recent_reports"],
            "language_stats": processed_data["language_stats"],
            "last_updated": datetime.now().isoformat()
        }

        with open(api_dir / "summary.json", 'w', encoding='utf-8') as f:
            json.dump(api_data, f, ensure_ascii=False, indent=2)

        logger.info("âœ… APIæ•°æ®æ–‡ä»¶ç”Ÿæˆå®Œæˆ")

    def _generate_seo_files(self, processed_data: Dict[str, Any]):
        """ç”ŸæˆSEOç›¸å…³æ–‡ä»¶"""
        # robots.txt
        robots_content = """User-agent: *
Allow: /

Sitemap: /sitemap.xml
"""
        with open(self.output_dir / "robots.txt", 'w', encoding='utf-8') as f:
            f.write(robots_content)

        # sitemap.xml
        sitemap_content = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>/</loc>
        <changefreq>daily</changefreq>
        <priority>1.0</priority>
    </url>
    <url>
        <loc>/archive.html</loc>
        <changefreq>daily</changefreq>
        <priority>0.8</priority>
    </url>
    <url>
        <loc>/about.html</loc>
        <changefreq>monthly</changefreq>
        <priority>0.5</priority>
    </url>
"""

        # æ·»åŠ æŠ¥å‘Šé¡µé¢
        for report in processed_data["recent_reports"]:
            sitemap_content += f"""    <url>
        <loc>{report['url']}</loc>
        <changefreq>weekly</changefreq>
        <priority>0.7</priority>
    </url>
"""

        sitemap_content += "</urlset>"

        with open(self.output_dir / "sitemap.xml", 'w', encoding='utf-8') as f:
            f.write(sitemap_content)

        logger.info("âœ… SEOæ–‡ä»¶ç”Ÿæˆå®Œæˆ")

    def _load_reports_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½å†å²æŠ¥å‘Šæ•°æ®"""
        # è¿™é‡Œåº”è¯¥ä»å®é™…çš„æ•°æ®å­˜å‚¨ä¸­åŠ è½½
        # ä¸´æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []

    def get_output_size(self) -> str:
        """è·å–è¾“å‡ºç›®å½•å¤§å°"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(self.output_dir):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    total_size += os.path.getsize(filepath)

            # è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼
            if total_size < 1024:
                return f"{total_size}B"
            elif total_size < 1024 * 1024:
                return f"{total_size / 1024:.1f}KB"
            else:
                return f"{total_size / (1024 * 1024):.1f}MB"
        except Exception as e:
            logger.error(f"è®¡ç®—è¾“å‡ºå¤§å°å¤±è´¥: {e}")
            return "æœªçŸ¥"