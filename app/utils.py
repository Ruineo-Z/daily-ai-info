"""å·¥å…·å‡½æ•°æ¨¡å—"""

import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger


def get_today_str() -> str:
    """è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸² YYYY-MM-DD"""
    return datetime.now().strftime("%Y-%m-%d")


def get_output_filename(date_str: str = None) -> str:
    """è·å–è¾“å‡ºæ–‡ä»¶å"""
    if not date_str:
        date_str = get_today_str()
    return f"{date_str}-ai-news.md"


def generate_content_hash(title: str, url: str) -> str:
    """ç”Ÿæˆå†…å®¹å“ˆå¸Œï¼Œç”¨äºå»é‡"""
    content = f"{title}|{url}"
    return hashlib.md5(content.encode()).hexdigest()[:8]


def clean_old_files(directory: Path, retention_days: int):
    """æ¸…ç†è¿‡æœŸæ–‡ä»¶"""
    if not directory.exists():
        return

    cutoff_date = datetime.now() - timedelta(days=retention_days)
    deleted_count = 0

    for file_path in directory.iterdir():
        if file_path.is_file():
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
            if file_mtime < cutoff_date:
                try:
                    file_path.unlink()
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤è¿‡æœŸæ–‡ä»¶: {file_path}")
                except Exception as e:
                    logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    if deleted_count > 0:
        logger.info(f"æ¸…ç† {directory} ç›®å½•ï¼Œåˆ é™¤ {deleted_count} ä¸ªè¿‡æœŸæ–‡ä»¶")


def format_markdown_section(title: str, items: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–Markdownæ®µè½"""
    if not items:
        return ""

    lines = [f"## {title}\n"]

    for i, item in enumerate(items, 1):
        name = item.get('name', item.get('title', 'Unknown'))
        url = item.get('url', item.get('link', '#'))
        description = item.get('description', item.get('summary', ''))

        # åŸºæœ¬ä¿¡æ¯è¡Œ
        lines.append(f"### {i}. [{name}]({url})")

        if description:
            lines.append(f"{description}")

        # é¢å¤–ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        extra_info = []
        if item.get('stars'):
            extra_info.append(f"â­ {item['stars']}")
        if item.get('language'):
            extra_info.append(f"ğŸ”¤ {item['language']}")
        if item.get('authors'):
            extra_info.append(f"ğŸ‘¨â€ğŸ’» {item['authors']}")

        if extra_info:
            lines.append(f"*{' | '.join(extra_info)}*")

        lines.append("")  # ç©ºè¡Œåˆ†éš”

    return "\n".join(lines)


def create_daily_report_header(date_str: str) -> str:
    """åˆ›å»ºæ¯æ—¥æŠ¥å‘Šå¤´éƒ¨"""
    return f"""# AIæŠ€æœ¯èµ„è®¯æ—¥æŠ¥ - {date_str}

> ç”±AIè‡ªåŠ¨æ”¶é›†å’Œæ€»ç»“ï¼Œæ¯æ—¥æ›´æ–°

**æ•°æ®æ¥æº**ï¼š
- GitHub Trending (AI/ML)
- Papers with Code
- Hugging Face Models
- arXiv AI Papers
- Towards Data Science

---

"""


def save_markdown_report(file_path: Path, content: str):
    """ä¿å­˜MarkdownæŠ¥å‘Š"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"æŠ¥å‘Šä¿å­˜æˆåŠŸ: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥ {file_path}: {e}")
        raise