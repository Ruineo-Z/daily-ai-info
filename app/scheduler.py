"""å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - æ¯å¤©æ—©ä¸Š8:30æ‰§è¡ŒGitHub Trendingåˆ†æ"""

import asyncio
from datetime import datetime
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.executors.asyncio import AsyncIOExecutor
from pytz import timezone
from loguru import logger
from .crawlers.github_trending_web import GitHubTrendingWebCrawler
from .ai_processor import AIProcessor
from .github_uploader import GitHubUploader
from .static_site_generator import StaticSiteGenerator
import os
import json


class DailyAIScheduler:
    """æ¯æ—¥AIèµ„è®¯å®šæ—¶è°ƒåº¦å™¨"""

    def __init__(self):
        # è®¾ç½®ä¸Šæµ·æ—¶åŒº
        self.shanghai_tz = timezone('Asia/Shanghai')

        # é…ç½®è°ƒåº¦å™¨
        executors = {
            'default': AsyncIOExecutor()
        }
        job_defaults = {
            'coalesce': False,  # ä¸åˆå¹¶ä½œä¸š
            'max_instances': 1  # åŒæ—¶æœ€å¤šä¸€ä¸ªå®ä¾‹
        }

        self.scheduler = AsyncIOScheduler(
            executors=executors,
            job_defaults=job_defaults,
            timezone=self.shanghai_tz
        )

        # åˆå§‹åŒ–é™æ€ç½‘ç«™ç”Ÿæˆå™¨
        self.site_generator = StaticSiteGenerator()

        logger.info("å®šæ—¶è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ—¶åŒºï¼šä¸Šæµ·")

    def generate_report_path(self):
        """ç”ŸæˆæŒ‰æ—¥æœŸç»„ç»‡çš„æŠ¥å‘Šè·¯å¾„"""
        now = datetime.now(self.shanghai_tz)
        year = now.strftime("%Y")
        month = now.strftime("%m")
        day = now.strftime("%d")
        time_str = now.strftime("%H%M")

        # æŒ‰æ—¥æœŸç»„ç»‡ç›®å½•ç»“æ„
        date_dir = os.path.join("data", year, month, day)
        os.makedirs(date_dir, exist_ok=True)

        filename = f"GitHub-Trending-AIåˆ†æ_{time_str}.md"
        filepath = os.path.join(date_dir, filename)

        return filename, filepath, date_dir

    def generate_markdown_report(self, summary_result: dict, projects: list) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        now = datetime.now(self.shanghai_tz)
        today_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")

        markdown = f"""# GitHub Trending AIæŠ€æœ¯åˆ†ææŠ¥å‘Š - {today_str}

> åŸºäºGitHub Daily Trendingçš„AIæŠ€æœ¯åŠ¨æ€æ·±åº¦åˆ†æ
>
> ğŸ• **ç”Ÿæˆæ—¶é—´**: {now.strftime('%Y-%m-%d %H:%M:%S')} (ä¸Šæµ·æ—¶åŒº)

## ğŸ“‹ ä»Šæ—¥æ‘˜è¦

{summary_result.get('summary', 'ä»Šæ—¥AIæŠ€æœ¯èµ„è®¯å·²æ•´ç†å®Œæˆ')}

"""

        # æŠ€æœ¯è¶‹åŠ¿
        trends = summary_result.get('trends', [])
        if trends:
            markdown += "## ğŸ“ˆ æ ¸å¿ƒæŠ€æœ¯è¶‹åŠ¿\n\n"
            for i, trend in enumerate(trends, 1):
                markdown += f"{i}. {trend}\n"
            markdown += "\n"

        # é¡¹ç›®è¯¦æƒ…
        markdown += f"## ğŸš€ GitHubçƒ­é—¨é¡¹ç›®æ·±åº¦è§£æ ({len(projects)}ä¸ª)\n\n"
        project_summaries = summary_result.get('project_summaries', [])

        for i, project in enumerate(projects, 1):
            name = project.get('name', 'Unknown')
            description = project.get('description', 'æš‚æ— æè¿°')
            url = project.get('url', '#')
            stars = project.get('stars', '0')
            language = project.get('language', 'Unknown')
            stars_today = project.get('stars_today', '')
            author = project.get('author', 'Unknown')

            markdown += f"### {i}. [{name}]({url})\n\n"

            # AIç”Ÿæˆçš„æŠ€æœ¯æ´å¯Ÿ
            if i <= len(project_summaries) and project_summaries[i-1]:
                ai_summary = project_summaries[i-1].strip()
                # æ¸…ç†å‰ç¼€
                prefixes = [f"{i}.", f"{i}ã€", "- ", "* ", f"**{name}**: ", f"**{name.split('/')[-1]}**: "]
                for prefix in prefixes:
                    if ai_summary.startswith(prefix):
                        ai_summary = ai_summary[len(prefix):].strip()
                        break
                markdown += f"**AIæŠ€æœ¯æ´å¯Ÿ**: {ai_summary}\n\n"
            else:
                markdown += f"**é¡¹ç›®ç®€ä»‹**: {description}\n\n"

            markdown += f"**GitHubæ•°æ®**:\n"
            markdown += f"- â­ **Stars**: {stars}"
            if stars_today:
                markdown += f" (ä»Šæ—¥+{stars_today})"
            markdown += f"\n- ğŸ’» **ä¸»è¯­è¨€**: {language}\n"
            markdown += f"- ğŸ‘¨â€ğŸ’» **ä½œè€…**: {author}\n\n"
            markdown += "---\n\n"

        # æ•°æ®ç»Ÿè®¡
        markdown += "## ğŸ“Š GitHub Trendingæ•°æ®åˆ†æ\n\n"
        markdown += f"- **åˆ†æé¡¹ç›®æ€»æ•°**: {len(projects)}\n"

        # ç¼–ç¨‹è¯­è¨€ç»Ÿè®¡
        languages = {}
        for project in projects:
            lang = project.get('language', 'Unknown')
            languages[lang] = languages.get(lang, 0) + 1

        top_languages = sorted(languages.items(), key=lambda x: x[1], reverse=True)[:7]
        markdown += f"- **ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ**: {', '.join([f'{lang}({count})' for lang, count in top_languages])}\n"

        # Starsæ´»è·ƒåº¦
        total_stars_today = 0
        for project in projects:
            stars_today_str = project.get('stars_today', '').replace(' stars today', '').replace(',', '')
            if stars_today_str.isdigit():
                total_stars_today += int(stars_today_str)

        if total_stars_today > 0:
            markdown += f"- **ä»Šæ—¥æ–°å¢Starsæ€»æ•°**: {total_stars_today:,}\n"

        # æŠ¥å‘Šä¿¡æ¯
        markdown += f"\n## ğŸ“‹ åˆ†ææŠ¥å‘Šä¿¡æ¯\n\n"
        markdown += f"- **æ•°æ®æ¥æº**: GitHub Trending (Daily)\n"
        markdown += f"- **æ‰§è¡Œæ—¶é—´**: {now.strftime('%Y-%m-%d %H:%M:%S')} (Asia/Shanghai)\n"
        markdown += f"- **AIåˆ†æå¼•æ“**: Google Gemini 2.5 Pro\n"
        markdown += f"- **åˆ†ææ·±åº¦**: é¡¹ç›®æè¿° + READMEå†…å®¹\n"
        markdown += f"- **è°ƒåº¦æ–¹å¼**: å®šæ—¶ä»»åŠ¡ (æ¯æ—¥8:30)\n"
        markdown += f"- **æŠ¥å‘Šæ ¼å¼**: Markdown v1.0\n"

        return markdown

    async def run_daily_analysis(self):
        """æ‰§è¡Œæ¯æ—¥AIæŠ€æœ¯åˆ†æä»»åŠ¡"""
        now = datetime.now(self.shanghai_tz)
        logger.info(f"ğŸ• å¼€å§‹æ‰§è¡Œæ¯æ—¥AIæŠ€æœ¯åˆ†æ - {now.strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            # æ­¥éª¤1: çˆ¬å–GitHub Trendingæ•°æ®
            logger.info("ğŸ” çˆ¬å–GitHub Trendingæ•°æ®")
            async with GitHubTrendingWebCrawler() as crawler:
                trending_data = await crawler.crawl(fetch_readme=True)

            if not trending_data:
                logger.error("âŒ æœªè·å–åˆ°trendingæ•°æ®ï¼Œä»»åŠ¡å¤±è´¥")
                return False

            logger.info(f"âœ… çˆ¬å–æˆåŠŸï¼Œè·å¾— {len(trending_data)} ä¸ªçƒ­é—¨é¡¹ç›®")

            # æ­¥éª¤2: AIå¤„ç†ä¸åˆ†æ
            logger.info("ğŸ¤– å¯åŠ¨AIåˆ†æå¼•æ“")
            ai_processor = AIProcessor()

            logger.info("ğŸ§¹ æ‰§è¡Œæ™ºèƒ½å»é‡")
            deduplicated_data = await ai_processor.deduplicate_by_titles(trending_data)

            logger.info("ğŸ“ ç”ŸæˆæŠ€æœ¯æ´å¯Ÿä¸è¶‹åŠ¿åˆ†æ")
            summary_result = await ai_processor.summarize_content(deduplicated_data)

            # æ­¥éª¤3: ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
            logger.info("ğŸ“‹ ç”Ÿæˆæ¯æ—¥åˆ†ææŠ¥å‘Š")
            markdown_content = self.generate_markdown_report(summary_result, deduplicated_data)

            filename, filepath, date_dir = self.generate_report_path()
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            # æ­¥éª¤4: ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆç”¨äºé™æ€ç½‘ç«™ï¼‰
            logger.info("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°æœ¬åœ°å­˜å‚¨")
            report_data = {
                "date": now.strftime("%Y-%m-%d"),
                "generation_time": now.strftime("%Y-%m-%d %H:%M:%S"),
                "summary": summary_result.get('summary', ''),
                "trends": summary_result.get('trends', []),
                "project_summaries": summary_result.get('project_summaries', []),
                "projects": deduplicated_data
            }

            # ä¿å­˜å½“å‰æŠ¥å‘Šæ•°æ®
            json_filepath = filepath.replace('.md', '.json')
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)

            # æ­¥éª¤5: ç”Ÿæˆé™æ€ç½‘ç«™
            logger.info("ğŸŒ ç”Ÿæˆé™æ€ç½‘ç«™")
            try:
                # åŠ è½½å†å²æ•°æ®
                historical_data = self._load_historical_reports()
                historical_data.append(report_data)

                # ç”Ÿæˆé™æ€ç½‘ç«™
                site_success = self.site_generator.generate_site(historical_data, summary_result)
                if site_success:
                    site_size = self.site_generator.get_output_size()
                    logger.info(f"âœ… é™æ€ç½‘ç«™ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {site_size}")
                else:
                    raise RuntimeError("é™æ€ç½‘ç«™ç”Ÿæˆå¤±è´¥")
            except Exception as e:
                logger.error(f"é™æ€ç½‘ç«™ç”Ÿæˆå¤±è´¥: {e}")
                raise RuntimeError(f"é™æ€ç½‘ç«™ç”Ÿæˆå¤±è´¥ï¼Œç”¨æˆ·æ— æ³•è®¿é—®æœ€æ–°å†…å®¹: {e}") from e

            # æ­¥éª¤6: ä¸Šä¼ åˆ°GitHubï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰
            github_url = None
            try:
                logger.info("ğŸ“¤ ä¸Šä¼ æŠ¥å‘Šåˆ°GitHub")
                github_uploader = GitHubUploader()
                github_url = await github_uploader.upload_daily_report(markdown_content, filepath)
                logger.info(f"ğŸŒ GitHubå¤‡ä»½æˆåŠŸ: {github_url}")
            except Exception as e:
                logger.warning(f"GitHubå¤‡ä»½å¤±è´¥ï¼ˆéè‡´å‘½é”™è¯¯ï¼‰: {e}")

            # è¾“å‡ºæ‰§è¡Œç»“æœ
            trends_count = len(summary_result.get('trends', []))
            logger.info("ğŸ‰ æ¯æ—¥AIæŠ€æœ¯åˆ†æå®Œæˆï¼")
            logger.info(f"ğŸ“Š æ‰§è¡Œç»“æœ:")
            logger.info(f"   - åˆ†æé¡¹ç›®: {len(deduplicated_data)} ä¸ª")
            logger.info(f"   - æŠ€æœ¯è¶‹åŠ¿: {trends_count} ä¸ª")
            logger.info(f"   - æŠ¥å‘Šç›®å½•: {date_dir}")
            logger.info(f"   - æŠ¥å‘Šæ–‡ä»¶: {filename}")
            logger.info(f"   - æ–‡ä»¶å¤§å°: {len(markdown_content):,} å­—ç¬¦")
            logger.info(f"   - é™æ€ç½‘ç«™å¤§å°: {self.site_generator.get_output_size()}")
            if github_url:
                logger.info(f"   - GitHubå¤‡ä»½: {github_url}")

            return True

        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥åˆ†æä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return False

    def _load_historical_reports(self) -> list:
        """åŠ è½½å†å²æŠ¥å‘Šæ•°æ®"""
        historical_data = []
        data_dir = os.path.join("data")

        if not os.path.exists(data_dir):
            return historical_data

        try:
            # éå†å¹´/æœˆ/æ—¥ç›®å½•ç»“æ„
            for year_dir in os.listdir(data_dir):
                year_path = os.path.join(data_dir, year_dir)
                if not os.path.isdir(year_path):
                    continue

                for month_dir in os.listdir(year_path):
                    month_path = os.path.join(year_path, month_dir)
                    if not os.path.isdir(month_path):
                        continue

                    for day_dir in os.listdir(month_path):
                        day_path = os.path.join(month_path, day_dir)
                        if not os.path.isdir(day_path):
                            continue

                        # æŸ¥æ‰¾JSONæ–‡ä»¶
                        for file in os.listdir(day_path):
                            if file.endswith('.json'):
                                json_path = os.path.join(day_path, file)
                                try:
                                    with open(json_path, 'r', encoding='utf-8') as f:
                                        report_data = json.load(f)
                                        historical_data.append(report_data)
                                except Exception as e:
                                    logger.warning(f"åŠ è½½å†å²æŠ¥å‘Šå¤±è´¥ {json_path}: {e}")

            # æŒ‰æ—¥æœŸæ’åº
            historical_data.sort(key=lambda x: x.get('date', ''), reverse=True)
            logger.info(f"åŠ è½½äº† {len(historical_data)} ä»½å†å²æŠ¥å‘Š")

        except Exception as e:
            logger.error(f"åŠ è½½å†å²æŠ¥å‘Šå¤±è´¥: {e}")

        return historical_data

    def add_daily_job(self):
        """æ·»åŠ æ¯æ—¥8:30çš„å®šæ—¶ä»»åŠ¡"""
        self.scheduler.add_job(
            func=self.run_daily_analysis,
            trigger=CronTrigger(
                hour=8,           # 8ç‚¹
                minute=30,        # 30åˆ†
                timezone=self.shanghai_tz
            ),
            id='daily_ai_analysis',
            name='æ¯æ—¥AIæŠ€æœ¯åˆ†æ',
            replace_existing=True
        )
        logger.info("âœ… å®šæ—¶ä»»åŠ¡å·²é…ç½®: æ¯å¤©8:30 (ä¸Šæµ·æ—¶åŒº) æ‰§è¡ŒAIæŠ€æœ¯åˆ†æ")

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.scheduler.start()
        logger.info("ğŸš€ å®šæ—¶è°ƒåº¦å™¨å·²å¯åŠ¨")

        # æ˜¾ç¤ºä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
        next_run = self.scheduler.get_job('daily_ai_analysis').next_run_time
        if next_run:
            next_run_shanghai = next_run.astimezone(self.shanghai_tz)
            logger.info(f"ğŸ“… ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_shanghai.strftime('%Y-%m-%d %H:%M:%S')} (ä¸Šæµ·æ—¶åŒº)")

    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.scheduler.shutdown()
        logger.info("ğŸ›‘ å®šæ—¶è°ƒåº¦å™¨å·²åœæ­¢")

    async def test_run(self):
        """æµ‹è¯•è¿è¡Œä¸€æ¬¡åˆ†æä»»åŠ¡"""
        logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡åˆ†æä»»åŠ¡")
        result = await self.run_daily_analysis()
        if result:
            logger.info("âœ… æµ‹è¯•è¿è¡ŒæˆåŠŸ")
        else:
            logger.error("âŒ æµ‹è¯•è¿è¡Œå¤±è´¥")
        return result